---
title: "TP3"
author: 
- Yanis Micha
- Andrew Hardwick
format:
 bookup-html+dark:
    toc: true
    toc-expand: 1
    toc-depth: 3
    toc-location: 'left'
    toc-title: 'Sommaire'
    number-sections: true
    highlight-style: pygments
    embed-resources: true
---

```{r,echo=FALSE,message=FALSE,warning=FALSE}
knitr::opts_chunk$set(eval=TRUE,echo= FALSE,message = FALSE,warning = FALSE)
library(ggplot2)
library(plotly)
library(gridExtra)
library(patchwork)
library(cluster)
library(FactoMineR)
library(factoextra)
library(kableExtra)
require(dbscan)
require(FNN)
require(visdat)
```

::: {.alert .alert-secondary role="alert"}
<a href="https://github.com/yanismicha/Apprentissage_stat" class="alert-link" style = "color:  #72afd2">Lien vers notre git</a>. Vous retrouverez l'ensemble des projets de cet UE sur ce git.
:::

# <u>Exercice 1 : k-means</u>

## 1: Chargement du jeu de donnée

```{r}
load("datasets/exo-1.Rdata")
kable(head(X))
kable(head(y))
```

## 2: kmeans

```{r,eval=FALSE}
?kmeans
```

Les arguments obligatoires dans la fonction [kmeans]{style="color: #8B0000;"}:

-   x: le jeu de données sur lequel on souhaite réaliser l'algorithme
-   centers: représente le nombre de clusters que l'on souhaite représenter lors de la construction de l'algorithme

## 3: clustering

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,echo=TRUE,eval=FALSE}
clust <- kmeans(X,3)
data <- data.frame(X,y)
data$cluster<- factor(clust$cluster)
ggplot(data, aes(x = Sepal.Length, y = Sepal.Width, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$centers), 
             aes(x = Sepal.Length, y = Sepal.Width), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$centers),
            aes(x = Sepal.Length, y = Sepal.Width),
            label = 1:3,
            color = "white", size = 4) +
  theme_light()
```

</details>

```{r}
clust <- kmeans(X,3)
data <- data.frame(X,y)
data$cluster<- factor(clust$cluster)
ggplot(data, aes(x = Sepal.Length, y = Sepal.Width, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$centers), 
             aes(x = Sepal.Length, y = Sepal.Width), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$centers),
            aes(x = Sepal.Length, y = Sepal.Width),
            label = 1:3,
            color = "white", size = 4) +
  theme_light()
```

## 4:nstart

L'argument **nstart** permet de spécifier le nombre de fois que l'algorithme va itérer avec différentes initialisations aléatoires des centroïdes.\
Il permet donc avec une valeur plus élevé d'optimiser l'algorithme des kmeans en réduisant l'impact de l'aléa du choix d'initialisation de nos centroïdes.\
Exemple avec k=5:

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,echo=TRUE,eval=FALSE}

for(i in c(1,5,10,20)){
  clust <- kmeans(X,5,nstart = i)
  data$cluster<- factor(clust$cluster)
  fig <- ggplot(data, aes(x = Sepal.Length, y = Sepal.Width, color = cluster)) +
    geom_point() +
    geom_point(data = data.frame(clust$centers), 
               aes(x = Sepal.Length, y = Sepal.Width), 
               color = "darkred",size = 6,shape= 15)+
    geom_text(data = data.frame(clust$centers),
              aes(x = Sepal.Length, y = Sepal.Width),
              label = 1:5,
              color = "white", size = 4) +
    labs(
      title = paste("nstart=",i)
    )
    theme_light()
  if(i == 5 | i == 20) {
    fig <- fig + guides(color = FALSE)
  }
  assign(paste0("fig", i), fig)
}
grid.arrange(fig1, fig5, fig10, fig20, ncol = 2)


```

</details>

```{r}

for(i in c(1,5,10,20)){
  clust <- kmeans(X,5,nstart = i)
  data$cluster<- factor(clust$cluster)
  fig <- ggplot(data, aes(x = Sepal.Length, y = Sepal.Width, color = cluster)) +
    geom_point() +
    geom_point(data = data.frame(clust$centers), 
               aes(x = Sepal.Length, y = Sepal.Width), 
               color = "darkred",size = 6,shape= 15)+
    geom_text(data = data.frame(clust$centers),
              aes(x = Sepal.Length, y = Sepal.Width),
              label = 1:5,
              color = "white", size = 4) +
    labs(
      title = paste("nstart=",i)
    )
    theme_light()
  if(i == 5 | i == 20) {
    fig <- fig + guides(color = FALSE)
  }
  assign(paste0("fig", i), fig)
}
grid.arrange(fig1, fig5, fig10, fig20, ncol = 2)


```

On peut voir qu'avec une seule itération, les clusters sont totalements différents qu'avec 5,10 et 20 itérations.\
Cela est dû comme expliqué précedemment au choix aléatoire des centroïdes à l'initialisation de l'algorithme. Entre 10 et 20 itérations, on peut observer des clusters quasiment à l'identique preuve que cela à permis de réduire cet aléa.\
On peut lier cet argument à **iter.max** qui permet de spécifier le nombre d'itération de chaque algorithme lancé avant l'arrêt de celui-çi.

## 5: critère de qualité de clustering

Le critère le plus connu permettant de juger la qualité d'un clustering est l'inertie. L'inertie mesure la dispersion des points de données par rapport à leurs centres de cluster. Plus l'inertie est faible, plus les clusters sont compacts et mieux le clustering est considéré. Autrement dit l'objectif est de minimiser la variance intra-cluster et maximiser la variance inter-cluster.\
Deux autres scores peuvent être intéréssant à utiliser:

-   le score **silhouette**: $$
    s_i=\frac{b_i-a_i}{max(b_i-a_i)}\\
    $$

Où $a_i$ est la distance moyenne du point avec tout les autres points dans le même cluster et où $b_i$ est la distance moyenne du point considéré avec tout les points du cluster le plus proche.\
- le score **gap statistic** qui utilise l'inertie et la compare à celles correspondants à des ensembles de données aléatoires. Il s'agit de la différence entre la valeur de l'inertie observée pour le clustering réel et la valeur moyenne de l'inertie pour les ensembles de données aléatoires. Plus cette différence est grande, plus le nombre de clusters est jugé approprié.

Visualisation de ces scores:

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}

ONC1 <-fviz_nbclust(data[,c(1,2)], FUNcluster = kmeans, method = "silhouette")+ggtitle("Score:Silhouette")
ONC2 <- fviz_nbclust(data[,c(1,2)], FUNcluster = kmeans, method = "wss")+ggtitle("Score:Inertie")
ONC3 <- fviz_nbclust(data[,c(1,2)], FUNcluster = kmeans, method = "gap_stat")+ggtitle("Score:Gap_stat")
(ONC1+ONC2)/ONC3

```

</details>

```{r}
ONC1 <-fviz_nbclust(data[,c(1,2)], FUNcluster = kmeans, method = "silhouette")+ggtitle("Score:Silhouette")
ONC2 <- fviz_nbclust(data[,c(1,2)], FUNcluster = kmeans, method = "wss")+ggtitle("Score:Inertie")
ONC3 <- fviz_nbclust(data[,c(1,2)], FUNcluster = kmeans, method = "gap_stat")+ggtitle("Score:Gap_stat")
(ONC1+ONC2)/ONC3

```

On voit qu'avec la technique du coude pour l'inertie et l'optimisation du gap_stat, on retrouve le même nombre de clusters optimaux: 3.\
En revanche le score silhouette donne une légère préférence pour deux clusters.\
Le choix de 3 clusters est plus que raisonnable pour la simple et bonne raison que notre jeu de données concerne 3 variétés de plantes différentes.

## 6 Taux d'agréement

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
clust <- kmeans(X,centers = 3,nstart = 10)
data$cluster <- as.character(clust$cluster)
cont <- table(data$y,data$cluster)
kable(cont)

# Définir une palette de couleurs pour la variable y (catégorie)
colors <- c("setosa"="darkred","versicolor"="darkorange","virginica"="steelblue")
# Visualisation des clusters avec fviz_cluster
  fviz_cluster(object = clust, X, geom = "point", stand = FALSE, show.clust.cent = FALSE, shape = 16,ellipse.alpha = 0.2)+
  # Ajout des points colorés par y
  geom_point(data = data, aes(x = Sepal.Length, y = Sepal.Width, color = factor(y)),shape=16) +
  # Suppression de la légende pour les clusters
    scale_color_discrete(guide = "none") +
    scale_color_manual(values = colors)+
  # titre
  labs(
    title = "kmeans 3 clusters avec taux d'agréement"
  )

```

</details>

```{r}
clust <- kmeans(X,centers = 3,nstart = 10)
data$cluster <- as.character(clust$cluster)
cont <- table(data$y,data$cluster)
kable(cont)

# Définir une palette de couleurs pour la variable y (catégorie)
colors <- c("setosa"="darkred","versicolor"="darkorange","virginica"="steelblue")
# Visualisation des clusters avec fviz_cluster
  fviz_cluster(object = clust, X, geom = "point", stand = FALSE, show.clust.cent = FALSE, shape = 16,ellipse.alpha = 0.2)+
  # Ajout des points colorés par y
  geom_point(data = data, aes(x = Sepal.Length, y = Sepal.Width, color = factor(y)),shape=16) +
  # Suppression de la légende pour les clusters
    scale_color_discrete(guide = "none") +
    scale_color_manual(values = colors)+
  # titre
  labs(
    title = "kmeans 3 clusters avec taux d'agréement"
  )

```

On peut voir sur ce graphique que le cluster1 représente parfaitement les setosa tandis que pour les deux autres clusters l'algorithme a eu plus de mal à les dissocier.

## 7: Introduction de nouvelles données

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
new_clust <- kmeans(rbind(X, X.test), 3,nstart = 10)
f1 <- fviz_cluster(object = clust, X, geom = "point", stand = FALSE, show.clust.cent = FALSE, shape = 16,ellipse.alpha = 0.2)+
  geom_point(data=data.frame(X.test),aes(x = Sepal.Length, y = Sepal.Width),shape=16)+
  labs(
    title= "Visualisation des nouveaux points"
  )
f2 <- fviz_cluster(object = new_clust, rbind(X,X.test), geom = "point", stand = FALSE, show.clust.cent = FALSE, shape = 16,ellipse.alpha = 0.2)+
  geom_point(data=data.frame(X.test),aes(x = Sepal.Length, y = Sepal.Width),shape=16)+
  labs(
    title= "Insertion des nouveaux points dans les clusters"
  )
f1/f2
```

</details>

```{r}
new_clust <- kmeans(rbind(X, X.test), 3,nstart = 10)
f1 <- fviz_cluster(object = clust, X, geom = "point", stand = FALSE, show.clust.cent = FALSE, shape = 16,ellipse.alpha = 0.2)+
  geom_point(data=data.frame(X.test),aes(x = Sepal.Length, y = Sepal.Width),shape=16)+
  labs(
    title= "Visualisation des nouveaux points"
  )
f2 <- fviz_cluster(object = new_clust, rbind(X,X.test), geom = "point", stand = FALSE, show.clust.cent = FALSE, shape = 16,ellipse.alpha = 0.2)+
  geom_point(data=data.frame(X.test),aes(x = Sepal.Length, y = Sepal.Width),shape=16)+
  labs(
    title= "Insertion des nouveaux points dans les clusters"
  )
f1/f2
```

# <u>Exercice 2 : Clustering et formes</u>

## Chargement des jeux de données

```{r}
Aggregation <- read.delim("datasets/Aggregation.txt", header=FALSE)
Aggregation<- Aggregation[order(Aggregation$V3), ]
spiral<- read.delim("datasets/3spiral.txt", header=FALSE,sep = " ")
spiral<- spiral[order(spiral$V3), ]
flame <- read.delim("datasets/flame.txt",header = FALSE)
flame<- flame[order(flame$V3), ]

```

## Aggregation

### CAH

Au vu du jeu de données on choisis k=7 clusters.\
On commence par utiliser la classification ascendante hiérachique pour tenter de discriminer les points en fonction des clusters attendus.\
Pour ce qui est des paramètres, après une pré-séléction intuituive et de l'essai-erreur, on choisis de garder la distance **euclidienne** et d'utiliser la méthode **single** pour la production du dendogramme.\
Mathématiquement, cette méthode se traduit comme la distance $D(X,Y)$ entre les clusters $X$ et $Y$: $$
D(X,Y) = \min_{x \in X, y \in Y} d(x,y)
$$ Nous obtenons alors les clusters suivants:

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
hclust <- hclust(dist(Aggregation),method = "single")
fviz_dend(hclust,repel = TRUE,k = 7,cex = 0.5,color_labels_by_k = FALSE, rect = TRUE)+ 
labs(title =  "Dendogramme")
clusters <- cutree(hclust, k = 7)
Aggregation$cluster <- clusters
ggplot(Aggregation, aes(x = V1, y = V2, color = factor(cluster))) +
  geom_point()+
  ggtitle("CLustering avec Hclust")+
  theme_light()
tab <- table(Aggregation$cluster,Aggregation$V3)
pourcentages <- numeric(length(unique(Aggregation$cluster))) # vecteur pour stocker les pourcentages
for (i in unique(Aggregation$cluster)) {
  cluster_total <- sum(tab[i, ]) # somme de la colonne correspondant au cluster i
  points_bien_clusterises <- tab[i, i] # nombre de points bien clusterisés pour le cluster i
  pourcentage <- points_bien_clusterises / cluster_total * 100 # calcul du pourcentage
  pourcentages[i] <- pourcentage # stockage du pourcentage dans le vecteur
}
cat("Pourcentage de points bien catégorisés pour chaque cluster:\n")
for(i in 1:length(pourcentages)){
  cat(paste("Dans le cluster",i,":",pourcentages[i],"\n"))
}
```

</details>

```{r,cache =TRUE}
hclust <- hclust(dist(Aggregation),method = "single")
fviz_dend(hclust,repel = TRUE,k = 7,cex = 0.5,color_labels_by_k = FALSE, rect = TRUE)+ 
labs(title =  "Dendogramme")
clusters <- cutree(hclust, k = 7)
Aggregation$cluster <- clusters
ggplot(Aggregation, aes(x = V1, y = V2, color = factor(cluster))) +
  geom_point()+
  ggtitle("CLustering avec Hclust")+
  theme_light()

tab <- table(Aggregation$cluster,Aggregation$V3)
pourcentages <- numeric(length(unique(Aggregation$cluster)))
for (i in unique(Aggregation$cluster)) {
  cluster_total <- sum(tab[i, ]) # somme de la colonne correspondant au cluster i
  points_bien_clusterises <- tab[i, i] # nombre de points bien clusterisés pour le cluster i
  pourcentage <- points_bien_clusterises / cluster_total * 100 # calcul du pourcentage
  pourcentages[i] <- pourcentage # stockage du pourcentage dans le vecteur
}
cat("Pourcentage de points bien catégorisés pour chaque cluster:\n")
for(i in 1:length(pourcentages)){
  cat(paste("Dans le cluster",i,":",pourcentages[i],"\n"))
}
```

Avec cette méthode, l'ensemble des points sont correctement catégorisé.

### Kmeans: Kcentroïdes vs Kmedoïdes

Nous comparons les Kcentroïdes vs Kmedoïdes avec une intuition qu'avec les Kcentroïdes, un problème inhérent à son algorithme risque de se passer sur les deux clusters du bas.\
Pour les paramètres, nous gardons bien evidemment k=7, et nous utilisons nstart=10 pour avoir une stabilité du résultat final:

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
clust <- kmeans(Aggregation,7,nstart= 10)

Aggregation$cluster<- factor(clust$cluster)
centroide <-ggplot(Aggregation, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$centers), 
             aes(x = V1, y = V2), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$centers),
            aes(x = V1, y = V2),
            label = unique(clust$cluster),
            color = "white", size = 4) +
  ggtitle("Kmeans avec centroïdes:")+
  theme_light()

  
clust<- pam(Aggregation,7,stand = TRUE,nstart = 10)  
Aggregation$cluster <- factor(clust$clustering)
medoide <- ggplot(Aggregation, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$medoids), 
             aes(x = V1, y = V2), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$medoids),
            aes(x = V1, y = V2),
            label = 1:7,
            color = "white", size = 4) +
  ggtitle("Kmeans avec medoïdes:")+
  theme_light()

centroide/medoide
```

</details>

```{r,cache =TRUE}
clust <- kmeans(Aggregation,7,nstart= 10)

Aggregation$cluster<- factor(clust$cluster)
centroide <-ggplot(Aggregation, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$centers), 
             aes(x = V1, y = V2), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$centers),
            aes(x = V1, y = V2),
            label = 1:7,
            color = "white", size = 4) +
  ggtitle("Kmeans avec centroïdes:")+
  theme_light()
  

  
clust<- pam(Aggregation,7,stand = TRUE,nstart = 10)  
Aggregation$cluster <- factor(clust$clustering)
medoide <- ggplot(Aggregation, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$medoids), 
             aes(x = V1, y = V2), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$medoids),
            aes(x = V1, y = V2),
            label = 1:7,
            color = "white", size = 4) +
  ggtitle("Kmeans avec medoïdes:")+
  theme_light()


centroide/medoide

```

La différence entre utilisation des k-centroïdes et des kmedoïdes reste assez mineure, le cluster 3 est séparé en deux avec les k-medoïdes mais n'est tout de même pas bien considéré.

L'algorithme kmeans et kmedoïdes a du mal à classifier les petits clusters qui sont proches de plus grand cluster, car la "masse" des plus gros cluster a tendance à attirer les centroïdes et medoïdes. De façon plus générale l'algorithme kmeans a du mâle à détecter des cluster qui ne sont pas de forme "circulaire/sphérique".

### kmeans clustering spectral

Afin de permettre au `kmeans`de mieux performer, nous allons transformer nos données en les segmentant à l'aide du clustering spectral. Il s'agit d'un algorithme de partitionnement des données reposant sur la théorie spectrale des graphes et l'algèbre linéaire.\
L'idée est de segmenter un graphe en plusieurs petits groupes ayant des valeurs similaires ou proches.

#### <u>Algorithme de Clustering Spectral</u>

Données : Ensemble de points $S = \{s_1, \ldots, s_n\}$ dans $\mathbb{R}^l$, nombre de clusters $k$

1.  **Formation de la matrice d'affinité** :
    a.  Calculer la matrice d'affinité $A$ selon $A_{ij} = \exp\left(-\frac{{||s_i - s_j||^2}}{{2 \sigma^2}}\right)$
2.  **Construction de la matrice de degré et de la matrice Laplacienne normalisée** :
    a.  Calculer la matrice diagonale $D$ où $D_{ii}$ est la somme de la $i$-ème ligne de $A$
    b.  Construire la matrice $L = D^{-1/2} A D^{-1/2}$
3.  **Calcul des vecteurs propres et formation de la matrice** $X$ :
    a.  Trouver les $k$ plus grands vecteurs propres de $L$
    b.  Assurer l'orthogonalité des vecteurs propres en cas de valeurs propres répétées
    c.  Former la matrice $X = [x_1, x_2, \ldots, x_k] \in \mathbb{R}^{n \times k}$ en empilant les vecteurs propres
4.  **Formation de la matrice** $Y$ :
    a.  Former la matrice $Y$ à partir de $X$ en normalisant chaque ligne de $X$ pour avoir une longueur unitaire : $Y_{ij} = \frac{{X_{ij}}}{{\sqrt{\sum_j (X_{ij})^2}}}$
5.  **Clustering des lignes de** $Y$ via k-means :
    a.  Utiliser k-means pour regrouper les lignes de $Y$ en $k$ clusters
6.  **Assignation des points d'origine aux clusters** :
    a.  Assigner chaque point d'origine $s_i$ au cluster $j$ si la ligne $i$ de $Y$ a été assignée au cluster $j$

On applique cet algorithme sur notre jeu de données,avec comme clusters $k=7$ et comme ecart-type<br> $\sigma$=`c(0.5,0.8,1,2)`:

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
spectral_clust<- function(data,k,sigma){
  # 1: on construit la matrice adjacente qui qynthétise les relations entre tous les points de données du graphe.
  A <- as.matrix(proxy::dist(data, function(si, sj) {exp(-sum((si - sj)^2) / (2 * sigma^2))}, auto_convert_data_frames = TRUE))
  # 2.1:on Calcul la matrice D qui calcul la somme des liens
  diagonal <- rowSums(A)
  D <- diag(diagonal)
  # 2.2: on calcul a matrice laplacienne
  L <- solve(sqrt(D))%*%A%*%solve(sqrt(D))
  # 3.1: on recupère les valeurs et vecteurs propres
  eigen_decomp <- eigen(L,symmetric = TRUE)
  # 3.2: on construit la matrice X
  X = eigen_decomp$vectors[,1:k]
  # 4: on normalise la matrice X
  Y <- t(apply(X, 1, function(row) row / sqrt(sum(row^2))))
  # 5: Application du kmeans sur la matrice Y
  kmeans_result <- kmeans(Y, centers = k)
  cluster_labels <- kmeans_result$cluster
  # on assigne nos points d'origine aux clusters
  original_data_clusters <- rep(NA, nrow(data))
  for (i in 1:length(cluster_labels)) {
    original_data_clusters[which(cluster_labels == i)] <- i
  }
   return(original_data_clusters)
}
 
Aggregation$cluster <- factor(spectral_clust(Aggregation[,c(1,2)],7,0.5))
ggplot(Aggregation, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  ggtitle("Kmeans avec clustering spectral")
```

</details>

```{r,cache =TRUE}
spectral_clust<- function(data,k,sigma){
  # 1: on construit la matrice adjacente qui qynthétise les relations entre tous les points de données du graphe.
  A <- as.matrix(proxy::dist(data, function(si, sj) {exp(-sum((si - sj)^2) / (2 * sigma^2))}, auto_convert_data_frames = TRUE))
  # 2.1:on Calcul la matrice D qui calcul la somme des liens
  diagonal <- rowSums(A)
  D <- diag(diagonal)
  # 2.2: on calcul a matrice laplacienne
  L <- solve(sqrt(D))%*%A%*%solve(sqrt(D))
  # 3.1: on recupère les valeurs et vecteurs propres
  eigen_decomp <- eigen(L,symmetric = TRUE)
  # 3.2: on construit la matrice X
  X = eigen_decomp$vectors[,1:k]
  # 4: on normalise la matrice X
  Y <- t(apply(X, 1, function(row) row / sqrt(sum(row^2))))
  # 5: Application du kmeans sur la matrice Y
  kmeans_result <- kmeans(Y, centers = k, nstart = 10)
  cluster_labels <- kmeans_result$cluster
  # on assigne nos points d'origine aux clusters
  original_data_clusters <- rep(NA, nrow(data))
  for (i in 1:length(cluster_labels)) {
    original_data_clusters[which(cluster_labels == i)] <- i
  }
   return(original_data_clusters)
}
 
Aggregation$cluster <- factor(spectral_clust(Aggregation[,c(1,2)],7,0.5))
ggplot(Aggregation, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  ggtitle("Kmeans avec clustering spectral")
```

Cet algorithme plus complexe serait très utile pour du clustering plus poussé. Pour ce jeu de données, on retiendra la simplicité avec le `CAH` avec comme methode linkage `single`qui fonctionne très bien!

## Flame

### CAH

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
hclust <- flashClust::flashClust(dist(flame),method = "ward")
clusters <- cutree(hclust, k = 2)
flame$cluster <- clusters
ggplot(flame, aes(x = V1, y = V2, color = factor(cluster))) +
  geom_point()+
  ggtitle("hclust")+
  theme_light()
tab <- table(flame$cluster,flame$V3)
pourcentages <- numeric(length(unique(flame$cluster)))
for (i in unique(flame$cluster)) {
  cluster_total <- sum(tab[,i ])# somme de la colonne correspondant au cluster i
  points_bien_clusterises <- tab[i, i] # nombre de points bien clusterisés pour le cluster i
  pourcentage <- points_bien_clusterises / cluster_total * 100 # calcul du pourcentage
  pourcentages[i] <- pourcentage # stockage du pourcentage dans le vecteur
}
cat("Pourcentage de points bien catégorisés pour chaque cluster:\n")
for(i in 1:length(pourcentages)){
  cat(paste("Dans le cluster",i,":",round(pourcentages[i],2),"\n"))
}
```

</details>

```{r,cache=TRUE}
hclust <- flashClust::flashClust(dist(flame),method = "ward")
#fviz_dend(hclust,repel = TRUE,k = 2,cex = 0.5,color_labels_by_k = FALSE, rect = TRUE)+ 
#labs(title =  "Dendogramme")
clusters <- cutree(hclust, k = 2)
flame$cluster <- clusters
ggplot(flame, aes(x = V1, y = V2, color = factor(cluster))) +
  geom_point()+
  ggtitle("hclust")+
  theme_light()
tab <- table(flame$cluster,flame$V3)
pourcentages <- numeric(length(unique(flame$cluster)))
for (i in unique(flame$cluster)) {
  cluster_total <- sum(tab[,i ])# somme de la colonne correspondant au cluster i
  points_bien_clusterises <- tab[i, i] # nombre de points bien clusterisés pour le cluster i
  pourcentage <- points_bien_clusterises / cluster_total * 100 # calcul du pourcentage
  pourcentages[i] <- pourcentage # stockage du pourcentage dans le vecteur
}
cat("Pourcentage de points bien catégorisés pour chaque cluster:\n")
for(i in 1:length(pourcentages)){
  cat(paste("Dans le cluster",i,":",round(pourcentages[i],2),"\n"))
}
```

L'algorithme n'arrive pas à bien trouver les deux clusters. On peut tenter de passer par des kmeans et des kmedoides en espérant un meilleur résultat.

### Kmeans

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
clust <- kmeans(flame,2,nstart = 10)
flame$cluster <- factor(clust$cluster)
centroide <- ggplot(flame, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$centers), 
             aes(x = V1, y = V2), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$centers),
            aes(x = V1, y = V2),
            label = 1:2,
            color = "white", size = 4) +
  ggtitle("Kmeans avec centroïdes:")+
  theme_light()
clust<- pam(flame,2,stand = TRUE,nstart = 10)  
flame$cluster <- factor(clust$clustering)
medoide <- ggplot(flame, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$medoids), 
             aes(x = V1, y = V2), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$medoids),
            aes(x = V1, y = V2),
            label = 1:2,
            color = "white", size = 4) +
  ggtitle("Kmeans avec medoïdes:")+
  theme_light()

centroide/medoide


```

</details>

```{r,cache =TRUE}
clust <- kmeans(flame[,c(1,2)],2,nstart = 10)
flame$cluster <- factor(clust$cluster)
centroide <- ggplot(flame, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$centers), 
             aes(x = V1, y = V2), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$centers),
            aes(x = V1, y = V2),
            label = 1:2,
            color = "white", size = 4) +
  ggtitle("Kmeans avec centroïdes:")+
  theme_light()
clust<- pam(flame,2,stand = TRUE,nstart = 10)  
flame$cluster <- factor(clust$clustering)
medoide <- ggplot(flame, aes(x = V1, y = V2, color = cluster)) +
  geom_point() +
  geom_point(data = data.frame(clust$medoids), 
             aes(x = V1, y = V2), 
             color = "darkred",size = 6,shape= 15)+
  geom_text(data = data.frame(clust$medoids),
            aes(x = V1, y = V2),
            label = 1:2,
            color = "white", size = 4) +
  ggtitle("Kmeans avec medoïdes:")+
  theme_light()

centroide/medoide


```

Très peu de différences au niveau des résultats de clustering entre les deux algorithmes pour ce jeu de donnée. On peut se tourner vers des techniques un peu plus complexe comme le DBscan ou le spectralclustering.

### DBscan

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de clustering qui identifie des zones de densité élevée dans l'espace des données, délimitant ainsi des clusters de formes arbitraires, tout en distinguant les points isolés comme du bruit.\
Pour cela, on observe d'abord pour chaque point, le nombre de voisins à au plus une distance de $\epsilon$ de celui-ci. Si l'observation considérée possède un certains nombre de voisins (en la comprenant) fixé au préalable, il s'agit d'une observation **coeur**. ON regroupe ensuite toute observation au voisinage d'une observation coeur en les considérant en un cluster. Les observations qui ne sont ni au voisinage d'une observation coeur, ni coeur elle même, sont considéré comme des outliers.\
Il convient alors de définir trois éléments pour cet algorithme:

-   la métrique utilisée pour calculer les voisinages: nous utliserons la distance euclidienne implémenté par défault dans la fonction `knn`.
-   le nombre de voisins nécessaire pour constituer une observation coeur. Nous pouvons faire varier cet argument avec au départ un nombre point minimal à trois au vu de notre jeu de données (2 outliers sont facilement visible à l'oeil).
-   la distance $\epsilon$ maximum entre voisins que nous définirons en visualisant les distances pour chaque observation de son voisin le plus proche.

Pour ce dernier argument, nous allons fixer $\epsilon$ tel qu'une part "sufisamment grande" des observations aient une distance à son plus proche voisin inférieure à $\epsilon$. Pour cela nous affichons sur un graphique les distances triés des voisins de chaque observations puis nous utilisons la méthode du coude:

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
k <- 3
knn_dist <- knn.dist(flame[,c(1,2)], k)
knn_dist_sorted <- apply(knn_dist, 2, sort)
elbow <- numeric(nrow(flame))
for(i in 1:nrow(flame)) {
  elbow[i] <- sum(knn_dist_sorted[i, 2:k]) / (k - 1)
}
epsilon_90_percent <- quantile(knn_dist_sorted[, k], 0.9)
ggplot(data.frame(x= 1:nrow(flame),y = elbow),aes(x = x, y = y))+
  geom_line(color="steelblue")+
  geom_hline(yintercept = epsilon_90_percent, linetype = "dashed", color = "darkred") +
  annotate("text", x = nrow(flame)-15, y = epsilon_90_percent, 
           label = paste("ε =", round(epsilon_90_percent, 2)), 
           hjust = -0.1, vjust = 1, color = "darkred") +
  xlab("Points triés") +
  ylab("Elbow Criterion") +
  theme_minimal()
```

</details>

```{r}
k <- 3
knn_dist <- knn.dist(flame[,c(1,2)], k)
knn_dist_sorted <- apply(knn_dist, 2, sort)
elbow <- numeric(nrow(flame))
for(i in 1:nrow(flame)) {
  elbow[i] <- sum(knn_dist_sorted[i, 2:k]) / (k - 1)
}
epsilon_90_percent <- quantile(knn_dist_sorted[, k], 0.9)
ggplot(data.frame(x= 1:nrow(flame),y = elbow),aes(x = x, y = y))+
  geom_line(color="steelblue")+
  geom_hline(yintercept = epsilon_90_percent, linetype = "dashed", color = "darkred") +
  annotate("text", x = nrow(flame)-15, y = epsilon_90_percent, 
           label = paste("ε =", round(epsilon_90_percent, 2)), 
           hjust = -0.1, vjust = 1, color = "darkred") +
  xlab("Points triés") +
  ylab("Elbow Criterion") +
  theme_minimal()
```

Avec cette méthode, on choisi un $\epsilon$ proche de 1.\
Il nous reste plus qu'à utiliser `dbscan` avec les arguments choisis et de visualiser graphiquement les clusters. COmme ce qui nous intéresse est d'avoir seulement 2 clusters, je n'affiche que les résultats qui ne font ressortir que deux clusters:

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
for(i in 3:8){
  dbscan_result <- dbscan(flame[,c(1,2)], eps =0.99, minPts = i,borderPoints = TRUE)
  num_clusters <- length(unique(dbscan_result$cluster[dbscan_result$cluster != 0]))
  if(num_clusters==2){
    flame$cluster <- factor(dbscan_result$cluster)
    fig <- ggplot(flame, aes(x = V1, y = V2, color = cluster)) +
      geom_point() +
      ggtitle(paste("dbscan avec voisins minimum=",i))
    print(fig)
  }
}
```

</details>

```{r,cache =TRUE}
for(i in 3:8){
  dbscan_result <- dbscan(flame[,c(1,2)], eps =0.99, minPts = i,borderPoints = TRUE)
  num_clusters <- length(unique(dbscan_result$cluster[dbscan_result$cluster != 0]))
  if(num_clusters==2){
    flame$cluster <- factor(dbscan_result$cluster)
    fig <- ggplot(flame, aes(x = V1, y = V2, color = cluster)) +
      geom_point() +
      ggtitle(paste("dbscan avec voisins minimum=",i))
    print(fig)
  }
}
#hdb <-  hdbscan(flame[,c(1,2)], minPts = 6)
```

Les points rouges (cluster 0) sont des **outliers**. Avec 6 voisins minimum, l'algorithme arrive assez bien à retranscrire les deux clusters même si 6 points sont considérés outliers aux bords des deux clusters.

### Kmeans avec clustering spectral

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
flame$cluster <- factor(spectral_clust(flame[,c(1,2)],2,1))
ggplot(flame, aes(x = V1, y = V2, color = cluster)) +
  geom_point() 
```

</details>

```{r,cache=TRUE}
spectral_clust<- function(data,k,sigma){
  # 1: on construit la matrice adjacente qui qynthétise les relations entre tous les points de données du graphe.
  A <- as.matrix(proxy::dist(data, function(si, sj) {exp(-sum((si - sj)^2) / (2 * sigma^2))}, auto_convert_data_frames = TRUE))
  # 2.1:on Calcul la matrice D qui calcul la somme des liens
  diagonal <- rowSums(A)
  D <- diag(diagonal)
  # 2.2: on calcul a matrice laplacienne
  L <- solve(sqrt(D))%*%A%*%solve(sqrt(D))
  # 3.1: on recupère les valeurs et vecteurs propres
  eigen_decomp <- eigen(L,symmetric = TRUE)
  # 3.2: on construit la matrice X
  X = eigen_decomp$vectors[,1:k]
  # 4: on normalise la matrice X
  Y <- t(apply(X, 1, function(row) row / sqrt(sum(row^2))))
  # 5: Application du kmeans sur la matrice Y
  kmeans_result <- kmeans(Y, centers = k, nstart = 10)
  cluster_labels <- kmeans_result$cluster
  # on assigne nos points d'origine aux clusters
  original_data_clusters <- rep(NA, nrow(data))
  for (i in 1:length(cluster_labels)) {
    original_data_clusters[which(cluster_labels == i)] <- i
  }
   return(original_data_clusters)
}
 
flame$cluster <- factor(spectral_clust(flame[,c(1,2)],2,1))
ggplot(flame, aes(x = V1, y = V2, color = cluster)) +
  geom_point() 
```

## Spiral

### CAH

On fait le même choix que pour le jeu de données `Aggregation` en prenant comme distance la distance euclidienne et comme méthode de linkage `single`:

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
hclust <- hclust(dist(spiral),method = "single")
fviz_dend(hclust,repel = TRUE,k = 7,cex = 0.5,color_labels_by_k = FALSE, rect = TRUE)+ 
labs(title =  "Dendogramme")
clusters <- cutree(hclust, k = 7)
Aggregation$cluster <- clusters
ggplot(Aggregation, aes(x = V1, y = V2, color = factor(cluster))) +
  geom_point()+
  ggtitle("CLustering avec Hclust")+
  theme_light()
tab <- table(Aggregation$cluster,Aggregation$V3)
pourcentages <- numeric(length(unique(Aggregation$cluster))) # vecteur pour stocker les pourcentages
for (i in unique(Aggregation$cluster)) {
  cluster_total <- sum(tab[i, ]) # somme de la colonne correspondant au cluster i
  points_bien_clusterises <- tab[i, i] # nombre de points bien clusterisés pour le cluster i
  pourcentage <- points_bien_clusterises / cluster_total * 100 # calcul du pourcentage
  pourcentages[i] <- pourcentage # stockage du pourcentage dans le vecteur
}
cat("Pourcentage de points bien catégorisés pour chaque cluster:\n")
for(i in 1:length(pourcentages)){
  cat(paste("Dans le cluster",i,":",pourcentages[i],"\n"))
}
```

</details>

```{r,cache=TRUE}
hclust <- hclust(dist(spiral),method = "single")
fviz_dend(hclust,repel = TRUE,k = 7,cex = 0.5,color_labels_by_k = FALSE, rect = TRUE)+ 
labs(title =  "Dendogramme")
clusters <- cutree(hclust, k = 3)
spiral$cluster <- clusters
ggplot(spiral, aes(x = V1, y = V2, color = factor(cluster))) +
  geom_point()+
  ggtitle("CLustering avec Hclust")+
  theme_light()
tab <- table(spiral$cluster,spiral$V3)
pourcentages <- numeric(length(unique(spiral$cluster))) # vecteur pour stocker les pourcentages
for (i in unique(spiral$cluster)) {
  cluster_total <- sum(tab[i, ]) # somme de la colonne correspondant au cluster i
  points_bien_clusterises <- tab[i, i] # nombre de points bien clusterisés pour le cluster i
  pourcentage <- points_bien_clusterises / cluster_total * 100 # calcul du pourcentage
  pourcentages[i] <- pourcentage # stockage du pourcentage dans le vecteur
}
cat("Pourcentage de points bien catégorisés pour chaque cluster:\n")
for(i in 1:length(pourcentages)){
  cat(paste("Dans le cluster",i,":",pourcentages[i],"\n"))
}

```

L'algorithme arrive parfaitement a retrouver les bons clusters. On peut tout de même par curiosité tester le spectral clustering pour tenter de faire fonctionner les kmeans.

### Kmeans avec clustering spectral

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
# Avec variance = 1 et 3 clusters
spiral$cluster <-factor(spectral_clust(spiral[,c(1,2)],3,1))
ggplot(spiral, aes(x = V1, y = V2, color = cluster)) +
  geom_point()
```

</details>

```{r}
spectral_clust<- function(data,k,sigma){
  # 1: on construit la matrice adjacente qui qynthétise les relations entre tous les points de données du graphe.
  A <- as.matrix(proxy::dist(data, function(si, sj) {exp(-sum((si - sj)^2) / (2 * sigma^2))}, auto_convert_data_frames = TRUE))
  # 2.1:on Calcul la matrice D qui calcul la somme des liens
  diagonal <- rowSums(A)
  D <- diag(diagonal)
  # 2.2: on calcul a matrice laplacienne
  L <- solve(sqrt(D))%*%A%*%solve(sqrt(D))
  # 3.1: on recupère les valeurs et vecteurs propres
  eigen_decomp <- eigen(L,symmetric = TRUE)
  # 3.2: on construit la matrice X
  X = eigen_decomp$vectors[,1:k]
  # 4: on normalise la matrice X
  Y <- t(apply(X, 1, function(row) row / sqrt(sum(row^2))))
  # 5: Application du kmeans sur la matrice Y
  kmeans_result <- kmeans(Y, centers = k)
  cluster_labels <- kmeans_result$cluster
  # on assigne nos points d'origine aux clusters
  original_data_clusters <- rep(NA, nrow(data))
  for (i in 1:length(cluster_labels)) {
    original_data_clusters[which(cluster_labels == i)] <- i
  }
   return(original_data_clusters)
}
# Avec variance = 1 et 3 clusters
spiral$cluster <-factor(spectral_clust(spiral[,c(1,2)],3,1))
ggplot(spiral, aes(x = V1, y = V2, color = cluster)) +
  geom_point()
```

# <u>Exercice 3: Analyse exporatoire</u>

```{r}
spectra <- read.table("datasets/spectra.txt")
meta <- read.table("datasets/meta-data.txt",header = TRUE)
names(meta) <- c("Espèces","Gènes")
```

## 1: représentation des observations

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
ggplot(meta,aes(x = Gènes,fill= Espèces))+
  geom_bar()+
  labs(
    y = "Quantité"
  )
```

</details>

```{r,cache=TRUE}
ggplot(meta,aes(x = Gènes,fill= Espèces))+
  geom_bar()+
  labs(
    y = "Quantité"
  )
```

## 2: ACP

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
spectra$Gènes<- meta$Gènes
ACP_res <- PCA(spectra,graph = FALSE,quali.sup = "Gènes")
dim1 <- fviz_pca_ind(ACP_res,habillage = "Gènes",geom.ind = "point",addEllipses = TRUE) +
  labs(
    title = "Représention de l'ACP sur l'axe 1 et 2"
  )
dim1
dim3 <- fviz_pca_ind(ACP_res,habillage = "Gènes",geom.ind = "point",addEllipses = TRUE,axes = 3:4) +
  labs(
    title = "Représention de l'ACP sur l'axe 3 et 4"
  )
dim3
```

</details>

```{r,cache=TRUE}
spectra$Gènes<- meta$Gènes
ACP_res <- PCA(spectra,graph = FALSE,quali.sup = "Gènes")
dim1 <- fviz_pca_ind(ACP_res,habillage = "Gènes",geom.ind = "point",addEllipses = TRUE) +
  labs(
    title = "Représention de l'ACP sur l'axe 1 et 2"
  )
dim1
dim3 <- fviz_pca_ind(ACP_res,habillage = "Gènes",geom.ind = "point",addEllipses = TRUE,axes = 3:4) +
  labs(
    title = "Représention de l'ACP sur l'axe 3 et 4"
  )
dim3
```

On peut voir que sur la représentation sur l'axe 3 et 4, les points sont plus centrés autour des deux axes. On peut plus facilement visualiser des groupes entre les différents gènes.\
Malgré cela, la variance représenté par les quatres premières composantes reste dérisoire (`r round(ACP_res$eig[4,3],0)`% de variance cumulée), on ne peut donc pas tirer grand chose de ces graphiques.

## 3: clustering hierarchique

```{r,eval=FALSE}
vis_dat(spectra)
```

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
hclust <- hclust(dist(spectra),method = "ward.D")
fviz_dend(hclust,repel = TRUE,k = 8,cex = 0.5,color_labels_by_k = FALSE, rect = TRUE)+ 
labs(title =  "Dendogramme")
clusters <- cutree(hclust, k = 8)
spectra$cluster <- clusters
tab <- table(clusters,meta$Gènes)
data <- as.data.frame.table(tab)
names(data) <-c("clusters","genes","freq") 
ggplot(data, aes(x = genes, y = freq, fill = factor(clusters))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Gènes", y = "Quantité", fill = "Clusters", title = "Quantité par cluster en fonction des gènes:CAH") +
  theme_minimal()
```

</details>

```{r,cache=TRUE, warning=FALSE}
hclust <- hclust(dist(spectra),method = "ward.D")
fviz_dend(hclust,repel = TRUE,k = 8,cex = 0.5,color_labels_by_k = FALSE, rect = TRUE)+ 
labs(title =  "Dendogramme")
clusters <- cutree(hclust, k = 8)
spectra$cluster <- clusters
tab <- table(clusters,meta$Gènes)
data <- as.data.frame.table(tab)
names(data) <-c("clusters","genes","freq") 
ggplot(data, aes(x = clusters, y = freq, fill = factor(genes))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Clusters", y = "Quantité", fill = "Gènes", title = "Quantité par cluster en fonction des gènes:CAH") +
  theme_minimal()
```

```{r}
kable(t(as.matrix(tab)), caption = "Résultat de la classification hiérarchique")
```

La stratégie "Ward" semble bien adapté et réussit à bien discriminer les clusters par gène (mis à part pour le gène `AUG` et `NYV`).\
Ces résultats sont tout à fait cohérent avec l'analyse en composantes principales sur les deux premières dimensions.

## 4: Kmeans

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
kmeans <- kmeans(spectra[,-c(ncol(spectra),ncol(spectra)-1)],centers = 8,nstart=10)
spectra$cluster <- as.character(kmeans$cluster)
cont <- table(meta$Gènes,spectra$cluster)
kable(cont, caption = "Résultat de la classification par kmeans")

# Visualisation des clusters
data <- as.data.frame.table(cont)
names(data) <-c("genes","clusters","freq") 
ggplot(data, aes(x = genes, y = freq, fill = factor(clusters))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Gènes", y = "Quantité", fill = "Clusters", title = "Quantité par cluster en fonction des gènes: kmeans") +
  theme_minimal()
```

</details>

```{r,cache =TRUE}
kmeans <- kmeans(spectra[,-c(ncol(spectra),ncol(spectra)-1)],centers = 8,nstart=10)
spectra$cluster <- as.character(kmeans$cluster)
cont <- table(meta$Gènes,spectra$cluster)
kable(cont, caption = "Résultat de la classification par kmeans")
```

```{r}
# Visualisation des clusters
data <- as.data.frame.table(cont)
names(data) <-c("genes","clusters","freq") 
ggplot(data, aes(x = clusters, y = freq, fill = factor(genes))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Clusters", y = "Quantité", fill = "Gènes", title = "Quantité par cluster en fonction des gènes: kmeans") +
  theme_minimal()
```

On peut remarquer que le clustering hiérarchique à une performance légèrement meilleure que le kmeans. Mais dans l'ensemble, les algorithmes semblent équivalents.

## 5: identification inter-espèce

### Gène RTO

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,echo=TRUE,eval=FALSE}
spectra$espèces <- meta$Espèces
ACP <- PCA(spectra[spectra$Gènes=="RTO",],quali.sup = c("Gènes","espèces","cluster"),graph = FALSE)
fviz_pca_ind(ACP,habillage = "espèces",addEllipses = TRUE)+
  ggtitle("Représentation ACP sur les composantes 1 et 2 du gène RTO")
```

</details>

```{r}
spectra$espèces <- meta$Espèces
ACP <- PCA(spectra[spectra$Gènes=="RTO",],quali.sup = c("Gènes","espèces","cluster"),graph = FALSE)
fviz_pca_ind(ACP,habillage = "espèces",addEllipses = TRUE)+
  ggtitle("Représentation ACP sur les composantes 1 et 2 du gène RTO")
```

Les observations des deux espèces s'entremêlent sur ce graphique ce qui ne permet pas une différenciation claire.

### Kmeans

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
data <- spectra[spectra$Gènes=="RTO",]
n <- ncol(spectra)
kmeans<- kmeans(data[,-c(n,n-1,n-2)],2,nstart=10)
data_plot <- data.frame(table(data$espèces,kmeans$cluster))
kable(data_plot)
names(data_plot) <-c("especes","clusters","freq") 
ggplot(data_plot, aes(x = especes, y = freq, fill = factor(clusters))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Espèces", y = "Quantité", fill = "Clusters", title = "Quantité par cluster en fonction des espèces: kmeans") +
  theme_minimal()
```

</details>

```{r}
data <- spectra[spectra$Gènes=="RTO",]
n <- ncol(spectra)
kmeans<- kmeans(data[,-c(n,n-1,n-2)],2,nstart=10)
data_plot <- data.frame(table(data$espèces,kmeans$cluster))
kable(data_plot)
names(data_plot) <-c("especes","clusters","freq") 
ggplot(data_plot, aes(x = especes, y = freq, fill = factor(clusters))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Espèces", y = "Quantité", fill = "Clusters", title = "Quantité par cluster en fonction des espèces: kmeans") +
  theme_minimal()
```

Il n'est pas possible avec un simple kmeans de discriminer parfaitement les deux espèces, il faudrait sûrement transformer nos données pour les visualiser sur plus de dimensions pour permettre une meilleure discrimination.

### Gène AUG

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
ACP <- PCA(spectra[spectra$Gènes=="AUG",],quali.sup = c("Gènes","espèces","cluster"),graph = FALSE)
fviz_pca_ind(ACP,habillage = "espèces",addEllipses = TRUE,geom.ind = "point")+
  ggtitle("Représentation ACP sur les composantes 1 et 2 du gène AUG")
```

</details>

```{r,cache=TRUE}
ACP <- PCA(spectra[spectra$Gènes=="AUG",],quali.sup = c("Gènes","espèces","cluster"),graph = FALSE)
fviz_pca_ind(ACP,habillage = "espèces",addEllipses = TRUE,geom.ind = "point")+
  ggtitle("Représentation ACP sur les composantes 1 et 2 du gène AUG")
```

On peut voir une nette séparation entre les observations des deux espèces.

<details>

<summary style="font-weight: bold; color: #72afd2;">

Voir le code

</summary>

```{r,eval=FALSE,echo=TRUE}
data <- spectra[spectra$Gènes=="AUG",]
n <- ncol(spectra)
kmeans<- kmeans(data[,-c(n,n-1,n-2)],2,nstart=10)
data_plot <- data.frame(table(data$espèces,kmeans$cluster))
kable(data_plot)
names(data_plot) <-c("especes","clusters","freq") 
ggplot(data_plot, aes(x = especes, y = freq, fill = factor(clusters))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Espèces", y = "Quantité", fill = "Clusters", title = "Quantité par cluster en fonction des espèces: kmeans") +
  theme_minimal()
```

</details>

```{r,cache=TRUE}
data <- spectra[spectra$Gènes=="AUG",]
n <- ncol(spectra)
kmeans<- kmeans(data[,-c(n,n-1,n-2)],2,nstart=10)
data_plot <- data.frame(table(data$espèces,kmeans$cluster))
kable(data_plot)
names(data_plot) <-c("especes","clusters","freq") 
ggplot(data_plot, aes(x = especes, y = freq, fill = factor(clusters))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Espèces", y = "Quantité", fill = "Clusters", title = "Quantité par cluster en fonction des espèces: kmeans") +
  theme_minimal()
```

On voit également qu'avec un simple kmeans, on arrive à facilement discriminer les deux types d'espèces.
