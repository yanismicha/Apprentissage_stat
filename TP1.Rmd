---
title: "TP1"
author: "Yanis Micha"
date: "15/01/2024"
output: html_document
---

# Exercice 1: Régression Linéaire
```{r}
load("regression-dataset.Rdata")
require(ggplot2)
require(MASS)
require(mclust)
require(FactoMineR)
require(factoextra)
```
## 2: Y en fonction de x:
```{r}
plot(lm(y~ x))
plot(x,y)
abline(lm(y~x),col= "red")
```

## 3: modèles polynomiaux
```{r}

x.grid = seq(min(x),max(x),by = 0.01)
deg.list=c(1:5,8,10,15,20)
MSE=c()
for(d in deg.list){
fit = lm( y ~ poly(x,d,raw = TRUE))
mse =mean(( y - fit$fitted.values)^2)
MSE=c(MSE,mse)
cat("*** degree",d,": MSE on training data=",mse,"***\n")
}
#fx.grid = predict(fit3,newdata = data.frame(("x"= x.test)))

plot(deg.list,MSE,type = "b",xlab = "polynomial degree",ylab = "MSE", main = "MSE vs Degree of polynomials")
```
## 4 Evaluation des modèles
```{r}
MSE.test=c()
for(d in deg.list){
fit = lm( y ~ poly(x,d,raw = TRUE))
preds = predict(fit, newdata = data.frame("x"=x.test))
mse =mean(( y.test - preds)^2)
MSE.test=c(MSE.test,mse)
cat("*** degree",d,": MSE on test data=",mse,"***\n")
}
plot(deg.list,ylim= range(c(MSE,MSE.test)),MSE,type = "b",xlab = "polynomial degree",ylab = "MSE",main="MSE vs degree")

```



# Exercice 4: LOI normale multivariée
##1 : génération
```{r}
mu= c(2,3)
sigma1 = diag(2)
sigma2 = matrix(c(4,0,0,1),ncol=2)
sigma3 = matrix(c(2,1,1,1),ncol=2)
sample1 = MASS::mvrnorm(n= 200,mu = mu,Sigma =sigma1)
sample2 = MASS::mvrnorm(n= 200,mu = mu,Sigma =sigma2)
sample3 = MASS::mvrnorm(n= 200,mu = mu,Sigma =sigma3)
par(mfrow = c(1,3))
xlim = range(c(sample1,sample2,sample3))
ylim = c(-2,7)
plot(sample1[,1],sample1[,2], xlim = xlim,ylim =ylim,xlab= "Loi normale(x)",ylab = "loi normale(y)",col= "steelblue",main = "spherical")
plot(sample2[,1],sample2[,2], xlim = xlim,ylim =ylim,xlab= "Loi normale(x)",ylab = "loi normale(y)",col= "steelblue",main = "diagonal")
plot(sample3[,1],sample3[,2], xlim = xlim,ylim =ylim, xlab= "Loi normale(x)",ylab = "loi normale(y)",col= "steelblue",main = "ellipsoidal")

```

## 2: Estimation des paramètres
```{r}
fit1 =mvn(modelName = "Spherical",sample1)
cat("**1 er model** t -estimated mean =", fit1$parameters$mean,"(vs mu =",mu,")\n")
cat("t -estimated Sigma =", fit1$parameters$variance$Sigma,"(vs Sigma =",sigma1,")\n")
fit2 =mvn(modelName = "Diagonal",sample2)
cat("**2 eme model** t -estimated mean =", fit2$parameters$mean,"(vs mu =",mu,")\n")
cat("t -estimated Sigma =", fit2$parameters$variance$Sigma,"(vs Sigma =",sigma2,")\n")

fit3 =mvn(modelName = "Ellipsoidal",sample3)
cat("**3 eme model** t -estimated mean =", fit3$parameters$mean,"(vs mu =",mu,")\n")
cat("t -estimated Sigma =", fit3$parameters$variance$Sigma,"(vs Sigma =",sigma3,")\n")




```
## 3 Problème en d dimensions
```{r}

```




## 4 


```{r}

```




# Exercice 2 :ACP
```{r}
require(FactoMineR)
require(factoextra)
```

```{r}
y = as.character(read.table("nci.label")$V1)
X = read.table("nci.data")
X= t(X)
t=sort(table(y),decreasing = TRUE)
barplot(t,las =2 ,main = "Nombre d'observations par classe")
```

## 2: 
```{r}
ind1 = grep("repro",y)
ind2 = which( t == "UNKNOWN")
ind.rm = c(ind1, ind2)
y = y[-ind.rm]
y = factor(y)
X = X[-ind.rm,]
```

## 3: ACP
```{r}
ACP <- PCA(X,graph = FALSE)
composantes <-nrow(ACP$eig)
cat("*** On obtient ",composantes," composantes  principales ***")
```
## 4
```{r}
fviz_pca_biplot(ACP,axes = 1:2,habillage = y)
pc1 <- ACP$ind$coord[, 1]
pc2 <- ACP$ind$coord[, 2]


df_plot <- data.frame(PC1 = pc1, PC2 = pc2, CancerType = y)

fviz_pca_ind(ACP, col.ind = df_plot$CancerType, palette = "jco",
             title = "Représentation des deux premières composantes principales")
```

## 5


## 6
```{r}
fviz_screeplot (ACP,addlabels=TRUE,ylim=c(0,15)) + geom_hline (yintercept = round(1 / (nrow (X) -1)*100,2), linetype = 2,
color = "red")
```
## 7
```{r}
ggplot(data.frame(Composante = seq_along(ACP$eig[, 3]), 
                  ValeurPropre = ACP$eig[, 3]), 
       aes(x = Composante, y = ValeurPropre)) +
  geom_bar(stat = "identity", fill = "steelblue", width = 0.3) +
  geom_hline(yintercept = 90,color="darkred")+
  labs(title = "Barplot cumulatif de la variance expliquée (en %)",
       x = "Composantes Principales",
       y = "Valeur Propre") +
  theme_minimal()
cat("*** 90% de la variance expliqué au bout de la 43 eme composante***")
cat("*** Avec 30 composantes, on peut expliquer environ",round(ACP$eig[30,3]),"% de la variance***")
```



# Exercice 3
```{r}
load("digits-3.Rdata")
```

```{r}
ind.grid
cols = gray(seq(1,0,length.out=256))
par(mfrow = c(n,n))
par(mar = c(1,1,1,1))
n <- 5
ind.sple <- sample(dim(I)[3],n*n)
for(i in 1:length(ind.sple)){
  image(I[,,i], col = cols)
}
```

```{r}
X <- apply(I,3,as.vector)
X <- t(X)
```

## ACP
```{r}
ACP <- PCA(X,graph = FALSE)
fviz_pca_biplot(ACP)
```

```{r}
eig <- get_eigenvalue(ACP)
I1 <- matrix(ACP$var$coord[,1],nrow = 16)
I2 <- matrix(ACP$var$coord[,2],nrow = 16)
par(mfrow = c(1,2))
image(I1, col = cols, main = "first principal component", axes = F)
box()
image(I2, col = cols, main = "second principal component", axes = F)
box()
```

```{r}
par(mfrow = c(5,5))
par(mar = c(1,1,1,1))
for(i in 1:dim(ind.grid)[1]){
  for(j in 1:dim(ind.grid)[2]){
    image(I[,,ind.grid[i,j]],col = cols)
    box()
  }
}
```




```{r}
coord <- data.frame(x = ACP$ind$coord[ind.grid,1],y =ACP$ind$coord[ind.grid,2])
fviz_pca_ind(ACP,geom = c("point"))+ geom_point(data = coord,aes(x = x, y = y,color= "darkred"))

```











